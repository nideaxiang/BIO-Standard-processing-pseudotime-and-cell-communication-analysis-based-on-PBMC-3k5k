{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ef2f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 7778 × 52238\n",
      "    obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'percent.mt', 'ident'\n",
      "    uns: 'X_name'\n",
      "    layers: 'logcounts'\n",
      "\n",
      "细胞元数据列名: ['orig.ident', 'nCount_RNA', 'nFeature_RNA', 'percent.mt', 'ident']\n",
      "矩阵 X 类型: <class 'scipy.sparse._csc.csc_matrix'>\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# 读取新生成的文件\n",
    "adata = sc.read_h5ad(\"pbmc_for_gpt_fixed.h5ad\")\n",
    "\n",
    "# 检查\n",
    "print(adata)\n",
    "print(f\"\\n细胞元数据列名: {adata.obs.columns.tolist()}\")\n",
    "print(f\"矩阵 X 类型: {type(adata.X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166843e2",
   "metadata": {},
   "source": [
    "n_obs × n_vars (7778 × 52238)：你有 7,778 个细胞和 52,238 个基因。基因数较多，说明这是包含全部基因的全转录组数据，非常适合 scGPT。\n",
    "矩阵 X 类型 (csc_matrix)：这是标准的稀疏矩阵格式，内存占用低，运行效率高。\n",
    "\n",
    "layers: 'logcounts'：这说明你的标准化数据被存放在了 layers 里，而默认的 X 通常存放的是原始计数（Counts）或者转换时指定的层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "906852b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 习惯性将 CSC 转为 CSR，因为 Scanpy 很多函数在 CSR 下更快\n",
    "adata.X = adata.X.tocsr()\n",
    "\n",
    "# 2. 确保 obs 和 var 的索引是字符串格式（防止某些工具报错）\n",
    "adata.obs_names = adata.obs_names.astype(str)\n",
    "adata.var_names = adata.var_names.astype(str)\n",
    "\n",
    "# 3. 备份原始数据到 .raw 槽位（scGPT 常用操作）\n",
    "adata.raw = adata\n",
    "\n",
    "# 4. 保存为最终版，之后 Python 分析直接读这个文件，不用再管 R 了\n",
    "adata.write_h5ad(\"pbmc_ready_for_scGPT.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21867c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.0+cu121\n",
      "Torchtext version: 0.16.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Torchtext version: {torchtext.__version__}\")\n",
    "\n",
    "# 测试 C++ 扩展是否能加载 (如果没有报错，就成功了)\n",
    "from torchtext._extension import _init_extension\n",
    "_init_extension()\n",
    "\n",
    "# 尝试导入 scGPT\n",
    "from scgpt.model import TransformerModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b0bb017",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tdc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscanpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msc\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtdc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tdc_hf_interface\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtdc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_server\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscgpt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scGPTTokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tdc'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "\n",
    "from tdc import tdc_hf_interface\n",
    "from tdc.model_server.tokenizers.scgpt import scGPTTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87737f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "\n",
    "# 1. 自动从 Hugging Face 下载模型文件夹\n",
    "model_dir = snapshot_download(repo_id=\"wanglab/scgpt-base\")\n",
    "print(f\"模型已下载至: {model_dir}\")\n",
    "\n",
    "# 2. 加载词表 (Vocab)\n",
    "vocab_path = os.path.join(model_dir, \"vocab.json\")\n",
    "vocab = GeneVocab.from_file(vocab_path)\n",
    "\n",
    "# 3. 实例化模型结构\n",
    "# wanglab/scgpt-base 的默认架构参数如下：\n",
    "model = TransformerModel(\n",
    "    ntokens=len(vocab),\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    d_hid=512,\n",
    "    nlayers=12,\n",
    "    nlayers_cls=3,\n",
    "    # 如果该模型支持预测表达量，可能还需要增加其它参数，但做 Embedding 这样足够了\n",
    ")\n",
    "\n",
    "# 4. 加载权重文件 (.pt 或 .bin)\n",
    "# 注意：文件名可能是 'best_model.pt' 或 'pytorch_model.bin'，请检查下载目录\n",
    "ckpt_path = os.path.join(model_dir, \"best_model.pt\")\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"模型加载成功！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "bio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
